const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');
const { v4: uuidv4 } = require('uuid');
const Redis = require('redis');
const Kubernetes = require('@kubernetes/client-node');
const Docker = require('dockerode');
const fs = require('fs');

const app = express();
const PORT = process.env.PORT || 3017;

// Redis client
const redis = Redis.createClient({
  url: process.env.REDIS_URL || 'redis://localhost:6379'
});

redis.on('error', (err) => console.log('Redis Client Error', err));
redis.connect();

// Kubernetes client
const k8s = new Kubernetes.KubeConfig();
k8s.loadFromDefault();
const k8sApi = k8s.makeApiClient(Kubernetes.CoreV1Api);
const k8sAppsApi = k8s.makeApiClient(Kubernetes.AppsV1Api);
const k8sAutoscalingApi = k8s.makeApiClient(Kubernetes.AutoscalingV2Api);

// Docker client
const docker = new Docker();

// Performance scaling configuration
const scalingConfig = {
  strategies: {
    'horizontal': {
      name: 'Horizontal Pod Autoscaler',
      description: 'Scale pods based on metrics',
      type: 'hpa',
      metrics: ['cpu', 'memory', 'custom']
    },
    'vertical': {
      name: 'Vertical Pod Autoscaler',
      description: 'Scale pod resources based on usage',
      type: 'vpa',
      metrics: ['cpu', 'memory']
    },
    'cluster': {
      name: 'Cluster Autoscaler',
      description: 'Scale cluster nodes based on demand',
      type: 'ca',
      metrics: ['pending_pods', 'resource_utilization']
    },
    'predictive': {
      name: 'Predictive Scaling',
      description: 'AI-powered predictive scaling',
      type: 'predictive',
      metrics: ['historical', 'trends', 'patterns']
    }
  },
  metrics: {
    'cpu': {
      name: 'CPU Utilization',
      unit: 'percentage',
      threshold: 70,
      scaleUp: 80,
      scaleDown: 30
    },
    'memory': {
      name: 'Memory Utilization',
      unit: 'percentage',
      threshold: 80,
      scaleUp: 90,
      scaleDown: 40
    },
    'requests': {
      name: 'Request Rate',
      unit: 'requests per second',
      threshold: 1000,
      scaleUp: 1500,
      scaleDown: 500
    },
    'latency': {
      name: 'Response Latency',
      unit: 'milliseconds',
      threshold: 500,
      scaleUp: 800,
      scaleDown: 200
    },
    'queue': {
      name: 'Queue Length',
      unit: 'messages',
      threshold: 100,
      scaleUp: 200,
      scaleDown: 50
    }
  },
  policies: {
    'aggressive': {
      name: 'Aggressive Scaling',
      scaleUpDelay: 30,
      scaleDownDelay: 300,
      scaleUpStabilization: 60,
      scaleDownStabilization: 300
    },
    'conservative': {
      name: 'Conservative Scaling',
      scaleUpDelay: 300,
      scaleDownDelay: 600,
      scaleUpStabilization: 300,
      scaleDownStabilization: 600
    },
    'balanced': {
      name: 'Balanced Scaling',
      scaleUpDelay: 60,
      scaleDownDelay: 300,
      scaleUpStabilization: 120,
      scaleDownStabilization: 300
    }
  }
};

// Middleware
app.use(helmet());
app.use(cors());
app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000,
  max: 100,
  message: 'Too many scaling requests, please try again later.'
});
app.use('/api/', limiter);

// Health check
app.get('/api/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: '3.0.0',
    uptime: process.uptime(),
    strategies: Object.keys(scalingConfig.strategies).length
  });
});

// Get scaling strategies
app.get('/api/strategies', (req, res) => {
  res.json(scalingConfig.strategies);
});

// Get scaling metrics
app.get('/api/metrics', (req, res) => {
  res.json(scalingConfig.metrics);
});

// Get scaling policies
app.get('/api/policies', (req, res) => {
  res.json(scalingConfig.policies);
});

// Configure auto-scaling
app.post('/api/configure', async (req, res) => {
  const { serviceName, strategy, metrics, policy, namespace } = req.body;
  
  if (!serviceName || !strategy || !metrics) {
    return res.status(400).json({ error: 'Service name, strategy, and metrics are required' });
  }
  
  try {
    const configId = uuidv4();
    const result = await configureAutoScaling(serviceName, strategy, metrics, policy, namespace, configId);
    
    res.json({
      configId,
      serviceName,
      strategy,
      result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Scale service
app.post('/api/scale', async (req, res) => {
  const { serviceName, replicas, namespace, strategy } = req.body;
  
  if (!serviceName || replicas === undefined) {
    return res.status(400).json({ error: 'Service name and replicas are required' });
  }
  
  try {
    const scaleId = uuidv4();
    const result = await scaleService(serviceName, replicas, namespace, strategy, scaleId);
    
    res.json({
      scaleId,
      serviceName,
      replicas,
      result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Get scaling status
app.get('/api/status/:serviceName', async (req, res) => {
  const { serviceName } = req.params;
  const { namespace } = req.query;
  
  try {
    const status = await getScalingStatus(serviceName, namespace);
    res.json(status);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Predictive scaling
app.post('/api/predictive', async (req, res) => {
  const { serviceName, historicalData, forecastPeriod, confidence } = req.body;
  
  if (!serviceName || !historicalData) {
    return res.status(400).json({ error: 'Service name and historical data are required' });
  }
  
  try {
    const predictiveId = uuidv4();
    const result = await configurePredictiveScaling(serviceName, historicalData, forecastPeriod, confidence, predictiveId);
    
    res.json({
      predictiveId,
      serviceName,
      result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Load balancing configuration
app.post('/api/load-balancer', async (req, res) => {
  const { serviceName, algorithm, weights, healthCheck, namespace } = req.body;
  
  if (!serviceName || !algorithm) {
    return res.status(400).json({ error: 'Service name and algorithm are required' });
  }
  
  try {
    const lbId = uuidv4();
    const result = await configureLoadBalancer(serviceName, algorithm, weights, healthCheck, namespace, lbId);
    
    res.json({
      lbId,
      serviceName,
      result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Performance monitoring
app.get('/api/monitor/:serviceName', async (req, res) => {
  const { serviceName } = req.params;
  const { namespace, timeframe } = req.query;
  
  try {
    const monitoring = await getPerformanceMonitoring(serviceName, namespace, timeframe);
    res.json(monitoring);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Scaling recommendations
app.post('/api/recommendations', async (req, res) => {
  const { serviceName, metrics, performanceData } = req.body;
  
  if (!serviceName || !metrics) {
    return res.status(400).json({ error: 'Service name and metrics are required' });
  }
  
  try {
    const recommendationId = uuidv4();
    const result = await generateScalingRecommendations(serviceName, metrics, performanceData, recommendationId);
    
    res.json({
      recommendationId,
      serviceName,
      result,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Scaling events
app.get('/api/events', async (req, res) => {
  const { serviceName, namespace, limit } = req.query;
  
  try {
    const events = await getScalingEvents(serviceName, namespace, limit);
    res.json(events);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Performance scaling functions
async function configureAutoScaling(serviceName, strategy, metrics, policy, namespace, configId) {
  const strategyConfig = scalingConfig.strategies[strategy];
  if (!strategyConfig) {
    throw new Error(`Invalid scaling strategy: ${strategy}`);
  }
  
  const policyConfig = scalingConfig.policies[policy] || scalingConfig.policies.balanced;
  
  // Create HPA configuration
  const hpa = {
    apiVersion: 'autoscaling/v2',
    kind: 'HorizontalPodAutoscaler',
    metadata: {
      name: serviceName,
      namespace: namespace || 'default'
    },
    spec: {
      scaleTargetRef: {
        apiVersion: 'apps/v1',
        kind: 'Deployment',
        name: serviceName
      },
      minReplicas: 1,
      maxReplicas: 10,
      metrics: metrics.map(metric => ({
        type: 'Resource',
        resource: {
          name: metric,
          target: {
            type: 'Utilization',
            averageUtilization: scalingConfig.metrics[metric]?.threshold || 70
          }
        }
      })),
      behavior: {
        scaleUp: {
          stabilizationWindowSeconds: policyConfig.scaleUpStabilization,
          policies: [{
            type: 'Pods',
            value: 2,
            periodSeconds: policyConfig.scaleUpDelay
          }]
        },
        scaleDown: {
          stabilizationWindowSeconds: policyConfig.scaleDownStabilization,
          policies: [{
            type: 'Pods',
            value: 1,
            periodSeconds: policyConfig.scaleDownDelay
          }]
        }
      }
    }
  };
  
  try {
    const result = await k8sAutoscalingApi.createNamespacedHorizontalPodAutoscaler(
      namespace || 'default',
      hpa
    );
    
    // Store configuration
    await redis.hSet('scaling_configs', configId, JSON.stringify({
      id: configId,
      serviceName,
      strategy,
      metrics,
      policy,
      namespace: namespace || 'default',
      hpa: result.body,
      timestamp: new Date().toISOString()
    }));
    
    return {
      success: true,
      hpa: result.body,
      message: `Auto-scaling configured for ${serviceName}`
    };
  } catch (error) {
    throw new Error(`Failed to configure auto-scaling: ${error.message}`);
  }
}

async function scaleService(serviceName, replicas, namespace, strategy, scaleId) {
  try {
    const result = await k8sAppsApi.patchNamespacedDeploymentScale(
      serviceName,
      namespace || 'default',
      { spec: { replicas } },
      undefined,
      undefined,
      undefined,
      undefined,
      { 'Content-Type': 'application/merge-patch+json' }
    );
    
    // Store scaling event
    await redis.hSet('scaling_events', scaleId, JSON.stringify({
      id: scaleId,
      serviceName,
      replicas,
      namespace: namespace || 'default',
      strategy: strategy || 'manual',
      timestamp: new Date().toISOString()
    }));
    
    return {
      success: true,
      deployment: result.body,
      message: `Service ${serviceName} scaled to ${replicas} replicas`
    };
  } catch (error) {
    throw new Error(`Failed to scale service: ${error.message}`);
  }
}

async function getScalingStatus(serviceName, namespace) {
  try {
    const hpa = await k8sAutoscalingApi.readNamespacedHorizontalPodAutoscaler(
      serviceName,
      namespace || 'default'
    );
    
    const deployment = await k8sAppsApi.readNamespacedDeployment(
      serviceName,
      namespace || 'default'
    );
    
    return {
      serviceName,
      namespace: namespace || 'default',
      hpa: hpa.body,
      deployment: deployment.body,
      status: {
        currentReplicas: deployment.body.spec.replicas,
        desiredReplicas: hpa.body.status.desiredReplicas || 0,
        minReplicas: hpa.body.spec.minReplicas,
        maxReplicas: hpa.body.spec.maxReplicas,
        scalingEnabled: true
      },
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    throw new Error(`Failed to get scaling status: ${error.message}`);
  }
}

async function configurePredictiveScaling(serviceName, historicalData, forecastPeriod, confidence, predictiveId) {
  // Simulate predictive scaling configuration
  const predictive = {
    id: predictiveId,
    serviceName,
    historicalData,
    forecastPeriod: forecastPeriod || '24 hours',
    confidence: confidence || 0.8,
    model: {
      type: 'LSTM',
      accuracy: Math.random() * 0.2 + 0.8,
      features: ['cpu', 'memory', 'requests', 'time_of_day', 'day_of_week']
    },
    predictions: {
      nextHour: Math.floor(Math.random() * 5) + 1,
      nextDay: Math.floor(Math.random() * 10) + 2,
      nextWeek: Math.floor(Math.random() * 20) + 5
    },
    recommendations: [
      {
        time: '09:00',
        action: 'scale_up',
        replicas: 5,
        reason: 'Expected traffic spike'
      },
      {
        time: '18:00',
        action: 'scale_down',
        replicas: 2,
        reason: 'Expected traffic decrease'
      }
    ],
    status: 'active',
    timestamp: new Date().toISOString()
  };
  
  // Store predictive configuration
  await redis.hSet('predictive_scaling', predictiveId, JSON.stringify(predictive));
  
  return predictive;
}

async function configureLoadBalancer(serviceName, algorithm, weights, healthCheck, namespace, lbId) {
  // Simulate load balancer configuration
  const loadBalancer = {
    id: lbId,
    serviceName,
    algorithm: algorithm || 'round_robin',
    weights: weights || {},
    healthCheck: healthCheck || {
      enabled: true,
      path: '/health',
      interval: 30,
      timeout: 10,
      threshold: 3
    },
    endpoints: [
      { host: 'pod1', port: 8080, weight: 1, healthy: true },
      { host: 'pod2', port: 8080, weight: 1, healthy: true },
      { host: 'pod3', port: 8080, weight: 1, healthy: false }
    ],
    statistics: {
      totalRequests: Math.floor(Math.random() * 100000) + 10000,
      averageLatency: Math.random() * 100 + 50,
      errorRate: Math.random() * 0.05,
      throughput: Math.random() * 1000 + 500
    },
    timestamp: new Date().toISOString()
  };
  
  // Store load balancer configuration
  await redis.hSet('load_balancers', lbId, JSON.stringify(loadBalancer));
  
  return loadBalancer;
}

async function getPerformanceMonitoring(serviceName, namespace, timeframe) {
  // Simulate performance monitoring
  const monitoring = {
    serviceName,
    namespace: namespace || 'default',
    timeframe: timeframe || '1 hour',
    metrics: {
      cpu: {
        current: Math.random() * 100,
        average: Math.random() * 80 + 20,
        peak: Math.random() * 100,
        trend: 'increasing'
      },
      memory: {
        current: Math.random() * 100,
        average: Math.random() * 70 + 30,
        peak: Math.random() * 100,
        trend: 'stable'
      },
      requests: {
        current: Math.random() * 2000 + 500,
        average: Math.random() * 1500 + 300,
        peak: Math.random() * 3000 + 1000,
        trend: 'increasing'
      },
      latency: {
        current: Math.random() * 500 + 100,
        average: Math.random() * 300 + 150,
        peak: Math.random() * 1000 + 200,
        trend: 'decreasing'
      }
    },
    scaling: {
      currentReplicas: Math.floor(Math.random() * 10) + 1,
      desiredReplicas: Math.floor(Math.random() * 10) + 1,
      lastScaling: new Date(Date.now() - Math.random() * 3600000).toISOString(),
      scalingEvents: Math.floor(Math.random() * 20) + 5
    },
    alerts: [
      {
        type: 'warning',
        message: 'High CPU usage detected',
        timestamp: new Date().toISOString()
      }
    ],
    timestamp: new Date().toISOString()
  };
  
  return monitoring;
}

async function generateScalingRecommendations(serviceName, metrics, performanceData, recommendationId) {
  // Simulate AI-powered scaling recommendations
  const recommendations = {
    id: recommendationId,
    serviceName,
    metrics,
    performanceData,
    recommendations: [
      {
        type: 'scale_up',
        priority: 'high',
        reason: 'CPU utilization consistently above 80%',
        action: 'Increase replicas from 3 to 5',
        expectedImpact: 'Reduce CPU utilization to 60%',
        confidence: 0.9
      },
      {
        type: 'scale_down',
        priority: 'medium',
        reason: 'Memory utilization below 40% during off-peak hours',
        action: 'Decrease replicas from 5 to 3 during 22:00-06:00',
        expectedImpact: 'Reduce costs by 40% during off-peak',
        confidence: 0.7
      },
      {
        type: 'optimize',
        priority: 'low',
        reason: 'Request latency increasing despite scaling',
        action: 'Optimize application code and database queries',
        expectedImpact: 'Reduce latency by 30%',
        confidence: 0.6
      }
    ],
    overallConfidence: Math.random() * 0.3 + 0.7,
    estimatedSavings: Math.random() * 1000 + 500,
    timestamp: new Date().toISOString()
  };
  
  // Store recommendations
  await redis.hSet('scaling_recommendations', recommendationId, JSON.stringify(recommendations));
  
  return recommendations;
}

async function getScalingEvents(serviceName, namespace, limit) {
  // Simulate scaling events
  const events = [];
  const eventCount = Math.min(limit || 50, 100);
  
  for (let i = 0; i < eventCount; i++) {
    events.push({
      id: uuidv4(),
      serviceName: serviceName || `service-${Math.floor(Math.random() * 5) + 1}`,
      namespace: namespace || 'default',
      type: Math.random() > 0.5 ? 'scale_up' : 'scale_down',
      replicas: Math.floor(Math.random() * 10) + 1,
      reason: Math.random() > 0.5 ? 'CPU threshold exceeded' : 'Memory threshold exceeded',
      timestamp: new Date(Date.now() - Math.random() * 86400000 * 7).toISOString()
    });
  }
  
  // Sort by timestamp (newest first)
  events.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
  
  return {
    events,
    total: events.length,
    serviceName: serviceName || 'all',
    namespace: namespace || 'all'
  };
}

// Error handling
app.use((err, req, res, next) => {
  console.error('Performance Scaling Error:', err);
  
  res.status(500).json({
    error: 'Internal Server Error',
    message: err.message,
    timestamp: new Date().toISOString()
  });
});

// 404 handler
app.use('*', (req, res) => {
  res.status(404).json({
    error: 'Endpoint not found',
    message: `Cannot ${req.method} ${req.originalUrl}`,
    timestamp: new Date().toISOString()
  });
});

// Start server
app.listen(PORT, () => {
  console.log(`üöÄ Performance Scaling Enhanced v3.0 running on port ${PORT}`);
  console.log(`üìà Advanced auto-scaling and load balancing enabled`);
  console.log(`ü§ñ AI-powered predictive scaling enabled`);
  console.log(`‚öñÔ∏è Intelligent load balancing enabled`);
  console.log(`üìä Performance monitoring and analytics enabled`);
  console.log(`üéØ Scaling recommendations enabled`);
});

module.exports = app;
